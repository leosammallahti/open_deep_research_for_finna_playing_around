# Deep Research Prompt: Optimizing TogetherAI for the Open Deep Research Project

## 1. Project Context

Our project, **Open Deep Research**, is an advanced, open-source AI assistant designed to automate in-depth research and generate comprehensive reports on any given topic. The system has two primary operational modes:

*   **Graph-based Workflow:** A structured, sequential process involving a `planner` agent to create a research plan, a `writer` agent to generate content for each section, a `reflection` agent to critique and improve upon the generated content, and a `summarizer` agent. This workflow allows for human-in-the-loop validation and is optimized for producing high-quality, accurate reports.
*   **Multi-Agent Workflow:** A parallelized system with a `supervisor` agent that breaks down the research task and multiple `researcher` agents that work simultaneously on different sections of the report. This workflow is optimized for speed and efficiency.

The project is built to be model-agnostic and leverages LangChain's `init_chat_model` for broad compatibility.

## 2. Current Implementation with TogetherAI

We are already using TogetherAI as our primary model provider. Our current configuration is as follows:

*   **Graph-based Workflow:**
    *   `planner_model`: `meta-llama/Llama-3.3-70B-Instruct-Turbo`
    *   `writer_model`: `meta-llama/Llama-3.3-70B-Instruct-Turbo`
*   **Multi-Agent Workflow:**
    *   `supervisor_model`: `together:meta-llama/Llama-3.3-70B-Instruct-Turbo`
    *   `researcher_model`: `together:meta-llama/Llama-3.3-70B-Instruct-Turbo`
    *   `summarization_model`: `Qwen/Qwen2.5-72B-Instruct-Turbo`

While functional, this configuration was chosen as a baseline. We believe there is significant room for optimization.

## 3. Research Objective

The goal of this research is to **identify opportunities to better utilize TogetherAI's offerings to improve our project's performance, cost-effectiveness, and the overall quality of the generated research reports.**

## 4. Key Research Questions

Please conduct a deep analysis of TogetherAI's platform and provide a detailed report that addresses the following questions:

### 4.1. Model Portfolio and Role-Based Optimization
- **Planner/Supervisor Role:** The `Llama-3.1-70B` model is a powerful reasoning engine. Are there other models on TogetherAI (e.g., from the DeepSeek family or other Llama variants) that could offer superior performance for planning, reasoning, and task decomposition at a similar or lower price point?
- **Writer/Researcher Role:** Content generation requires a balance of creativity, accuracy, and writing skill. How does `Llama-3.1-70B` compare to other models like Cohere's, or newer models from Qwen and Mistral available on the platform for long-form report writing?
- **Summarizer/Reflection Roles:** The `Qwen-1.5-72B-Chat` and `Llama-3.1-70B` models are very large for summarization and reflection tasks. Could smaller, faster, and cheaper models (e.g., Llama-3.1-8B, Mistral 7B, or Qwen-1.5-14B) perform these tasks effectively? Provide a cost-benefit analysis.

### 4.2. Cost and Performance Analysis
- Provide a detailed cost breakdown of our current model configuration versus a proposed, optimized configuration. Estimate potential monthly savings based on an assumed workload of 100 million tokens.
- Analyze any available data or benchmarks on the latency and throughput of the recommended models. How would the proposed changes impact the user experience, particularly the perceived speed of the research process?

### 4.3. Advanced Features: Fine-Tuning
- Investigate TogetherAI's fine-tuning services (both LoRA and full fine-tuning).
- **Feasibility:** Could we fine-tune a smaller, open-source model (e.g., `Llama-3.1-8B`) on a dataset of high-quality research reports generated by our current system?
- **Benefits:** What would be the potential benefits in terms of cost reduction and improved quality/consistency for the `writer` and `researcher` roles?
- **Process:** Outline the steps and estimated costs required to perform such a fine-tuning job on TogetherAI.

### 4.4. Implementation Plan
- Provide a clear, actionable plan for implementing the recommended model changes.
- This should include the specific model strings to be used in our `src/open_deep_research/configuration.py` file for an "Optimized" configuration setting.

Please synthesize your findings into a structured report that presents a clear, data-driven case for the proposed optimizations. 